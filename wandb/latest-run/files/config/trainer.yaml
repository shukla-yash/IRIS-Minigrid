wandb:
  mode: online
  project: iris
  entity: null
  name: null
  group: null
  tags: null
  notes: null
initialization:
  path_to_checkpoint: null
  load_tokenizer: false
  load_world_model: false
  load_actor_critic: false
common:
  epochs: 600
  device: cuda:0
  do_checkpoint: true
  seed: 0
  sequence_length: ${world_model.max_blocks}
  resume: false
collection:
  train:
    num_envs: 1
    stop_after_epochs: 500
    num_episodes_to_save: 10
    config:
      epsilon: 0.01
      should_sample: true
      temperature: 1.0
      num_steps: 200
      burn_in: ${training.actor_critic.burn_in}
  test:
    num_envs: 8
    num_episodes_to_save: ${collection.train.num_episodes_to_save}
    config:
      epsilon: 0.0
      should_sample: true
      temperature: 0.5
      num_episodes: 16
      burn_in: ${training.actor_critic.burn_in}
training:
  should: true
  learning_rate: 0.0001
  sampling_weights:
  - 0.125
  - 0.125
  - 0.25
  - 0.5
  tokenizer:
    batch_num_samples: 256
    grad_acc_steps: 1
    max_grad_norm: 10.0
    start_after_epochs: 5
    steps_per_epoch: 200
  world_model:
    batch_num_samples: 64
    grad_acc_steps: 1
    max_grad_norm: 10.0
    weight_decay: 0.01
    start_after_epochs: 25
    steps_per_epoch: 200
  actor_critic:
    batch_num_samples: 64
    grad_acc_steps: 1
    max_grad_norm: 10.0
    start_after_epochs: 50
    steps_per_epoch: 200
    imagine_horizon: ${common.sequence_length}
    burn_in: 20
    gamma: 0.995
    lambda_: 0.95
    entropy_weight: 0.001
evaluation:
  should: true
  every: 5
  tokenizer:
    batch_num_samples: ${training.tokenizer.batch_num_samples}
    start_after_epochs: ${training.tokenizer.start_after_epochs}
    save_reconstructions: true
  world_model:
    batch_num_samples: ${training.world_model.batch_num_samples}
    start_after_epochs: ${training.world_model.start_after_epochs}
  actor_critic:
    num_episodes_to_save: ${training.actor_critic.batch_num_samples}
    horizon: ${training.actor_critic.imagine_horizon}
    start_after_epochs: ${training.actor_critic.start_after_epochs}
tokenizer:
  _target_: models.tokenizer.Tokenizer
  vocab_size: 512
  embed_dim: 512
  encoder:
    _target_: models.tokenizer.Encoder
    config:
      _target_: models.tokenizer.EncoderDecoderConfig
      resolution: 64
      in_channels: 3
      z_channels: 512
      ch: 64
      ch_mult:
      - 1
      - 1
      - 1
      - 1
      - 1
      num_res_blocks: 2
      attn_resolutions:
      - 8
      - 16
      out_ch: 3
      dropout: 0.0
  decoder:
    _target_: models.tokenizer.Decoder
    config: ${..encoder.config}
world_model:
  _target_: models.TransformerConfig
  tokens_per_block: 17
  max_blocks: 20
  attention: causal
  num_layers: 10
  num_heads: 4
  embed_dim: 256
  embed_pdrop: 0.1
  resid_pdrop: 0.1
  attn_pdrop: 0.1
actor_critic:
  use_original_obs: false
env:
  train:
    _target_: envs.make_atari
    id: null
    size: 64
    max_episode_steps: 20000
    noop_max: 30
    frame_skip: 4
    done_on_life_loss: true
    clip_reward: false
  test:
    _target_: ${..train._target_}
    id: ${..train.id}
    size: ${..train.size}
    max_episode_steps: 500
    noop_max: 1
    frame_skip: ${..train.frame_skip}
    done_on_life_loss: false
    clip_reward: false
  keymap: atari/${.train.id}
datasets:
  train:
    _target_: dataset.EpisodesDatasetRamMonitoring
    max_ram_usage: 30G
    name: train_dataset
  test:
    _target_: dataset.EpisodesDataset
    max_num_episodes: null
    name: test_dataset
