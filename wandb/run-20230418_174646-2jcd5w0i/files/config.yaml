wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.12.7
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.10.4
    start_time: 1681840006
    t:
      1:
      - 1
      - 12
      3:
      - 16
      4: 3.10.4
      5: 0.12.7
      8:
      - 5
actor_critic:
  desc: null
  value:
    use_original_obs: false
collection:
  desc: null
  value:
    test:
      config:
        burn_in: 20
        epsilon: 0.0
        num_episodes: 16
        should_sample: true
        temperature: 0.5
      num_envs: 8
      num_episodes_to_save: 10
    train:
      config:
        burn_in: 20
        epsilon: 0.01
        num_steps: 200
        should_sample: true
        temperature: 1.0
      num_envs: 1
      num_episodes_to_save: 10
      stop_after_epochs: 500
common:
  desc: null
  value:
    device: cuda:0
    do_checkpoint: true
    epochs: 600
    resume: false
    seed: 0
    sequence_length: 20
datasets:
  desc: null
  value:
    test:
      _target_: dataset.EpisodesDataset
      max_num_episodes: null
      name: test_dataset
    train:
      _target_: dataset.EpisodesDatasetRamMonitoring
      max_ram_usage: 30G
      name: train_dataset
env:
  desc: null
  value:
    keymap: atari/None
    test:
      _target_: envs.make_atari
      clip_reward: false
      done_on_life_loss: false
      frame_skip: 4
      id: null
      max_episode_steps: 500
      noop_max: 1
      size: 64
    train:
      _target_: envs.make_atari
      clip_reward: false
      done_on_life_loss: true
      frame_skip: 4
      id: null
      max_episode_steps: 20000
      noop_max: 30
      size: 64
evaluation:
  desc: null
  value:
    actor_critic:
      horizon: 20
      num_episodes_to_save: 64
      start_after_epochs: 50
    every: 5
    should: true
    tokenizer:
      batch_num_samples: 256
      save_reconstructions: true
      start_after_epochs: 5
    world_model:
      batch_num_samples: 64
      start_after_epochs: 25
initialization:
  desc: null
  value:
    load_actor_critic: false
    load_tokenizer: false
    load_world_model: false
    path_to_checkpoint: null
tokenizer:
  desc: null
  value:
    _target_: models.tokenizer.Tokenizer
    decoder:
      _target_: models.tokenizer.Decoder
      config:
        _target_: models.tokenizer.EncoderDecoderConfig
        attn_resolutions:
        - 8
        - 16
        ch: 64
        ch_mult:
        - 1
        - 1
        - 1
        - 1
        - 1
        dropout: 0.0
        in_channels: 3
        num_res_blocks: 2
        out_ch: 3
        resolution: 64
        z_channels: 512
    embed_dim: 512
    encoder:
      _target_: models.tokenizer.Encoder
      config:
        _target_: models.tokenizer.EncoderDecoderConfig
        attn_resolutions:
        - 8
        - 16
        ch: 64
        ch_mult:
        - 1
        - 1
        - 1
        - 1
        - 1
        dropout: 0.0
        in_channels: 3
        num_res_blocks: 2
        out_ch: 3
        resolution: 64
        z_channels: 512
    vocab_size: 512
training:
  desc: null
  value:
    actor_critic:
      batch_num_samples: 64
      burn_in: 20
      entropy_weight: 0.001
      gamma: 0.995
      grad_acc_steps: 1
      imagine_horizon: 20
      lambda_: 0.95
      max_grad_norm: 10.0
      start_after_epochs: 50
      steps_per_epoch: 200
    learning_rate: 0.0001
    sampling_weights:
    - 0.125
    - 0.125
    - 0.25
    - 0.5
    should: true
    tokenizer:
      batch_num_samples: 256
      grad_acc_steps: 1
      max_grad_norm: 10.0
      start_after_epochs: 5
      steps_per_epoch: 200
    world_model:
      batch_num_samples: 64
      grad_acc_steps: 1
      max_grad_norm: 10.0
      start_after_epochs: 25
      steps_per_epoch: 200
      weight_decay: 0.01
wandb:
  desc: null
  value:
    entity: null
    group: null
    mode: online
    name: null
    notes: null
    project: iris
    tags: null
world_model:
  desc: null
  value:
    _target_: models.TransformerConfig
    attention: causal
    attn_pdrop: 0.1
    embed_dim: 256
    embed_pdrop: 0.1
    max_blocks: 20
    num_heads: 4
    num_layers: 10
    resid_pdrop: 0.1
    tokens_per_block: 17
